{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Diffusion Models\n",
        "Diffusion models are a type of generative model that model the data generation process as a Markov chain of diffusion steps. They gradually add noise to the data and learn to reverse this process to generate new samples. Diffusion models have several advantages over other generative models (VAE, GAN, ...), including improved sample quality (compared to VAE) and better mode coverage (compared to GAN). They are defined as a sequence of conditional Gaussian distributions, where each step adds or removes noise from the data. The key challenge in training diffusion models is to learn the reverse process, which requires optimizing a complex objective function. Despite this challenge, diffusion models have been extensively implemented in major models such as DALL-E, Imagen, and Stable Diffusion and are at the core of more recent video generative models such as Sora.\n",
        "\n",
        "We will cover the [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) *(Ho et al.)* approach in this notebook\n",
        "\n",
        "Some parts are adapted from this excellent [YouTube video](https://www.youtube.com/watch?v=a4Yfz2FxXiY)\n",
        "\n",
        "U-Net backbone architecture adapated from [this Github](https://github.com/milesial/Pytorch-UNet)"
      ],
      "metadata": {
        "id": "vFICNlBe-nwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "from torchvision.utils import make_grid"
      ],
      "metadata": {
        "id": "otxAYNwBXITn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "s779h5xiXGYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "num_epochs = 100\n",
        "batch_size = 8\n",
        "lr = 0.001\n",
        "\n",
        "train_ratio = 0.002   # ratio of training data to use\n",
        "test_ratio = 0.1\n",
        "img_size = 32\n",
        "\n",
        "T = 300               # diffusion steps"
      ],
      "metadata": {
        "id": "wR86ZIsuVpA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQnlc27k7Aiw"
      },
      "outputs": [],
      "source": [
        "# Some utils functions for later\n",
        "def plot_grid(noisy_imgs) :\n",
        "    noisy_grid = make_grid(noisy_imgs, nrow=8, padding=2, normalize=True)\n",
        "\n",
        "    # Convert the grid tensor to a numpy array\n",
        "    noisy_grid_np = noisy_grid.permute(1, 2, 0).detach().cpu().numpy()\n",
        "\n",
        "    # Display the grid of noisy images\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(noisy_grid_np)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def show_images(image):\n",
        "    reverse_transforms = transforms.Compose([\n",
        "        transforms.Lambda(lambda t: (t + 1) / 2),\n",
        "        transforms.Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
        "        transforms.Lambda(lambda t: t * 255.),\n",
        "        transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
        "        transforms.ToPILImage(),\n",
        "    ])\n",
        "\n",
        "    # Take first image of batch\n",
        "    if len(image.shape) == 4:\n",
        "        image = image[0, :, :, :]\n",
        "    plt.imshow(reverse_transforms(image))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forward Pass\n",
        "## Build the noise scheduler"
      ],
      "metadata": {
        "id": "Z8aGDDd9ZIpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to define a forward diffusion process by introducing a kernel $q$ that describes the transition probability from the data at the previous timestep ${x_{t-1}}$ to the data at the current timestep ${x_t}$. This kernel is defined as a conditional Gaussian distribution:\n",
        "\n",
        "$$q(x_t| x_{t-1}) = \\mathcal{N}(x_t;\\sqrt{1- \\beta_t} x_{t-1},\\beta_t I )$$\n",
        "\n",
        "Here, ${\\mathcal{N}(x;\\mu,\\sigma I )}$ represents a Gaussian distribution over $x$ with mean ${\\mu}$ and covariance matrix ${\\sigma I}$, where $I$ is the identity matrix. The notation ${y \\sim \\mathcal{N}(x;\\mu,\\sigma I )}$ means that $y$ is a sample drawn from this distribution given $x$. In other words, the distribution of $y$ given $x$ is ${\\mathcal{N}(\\mu,\\sigma I )}$.\n",
        "\n",
        "Let's first define the variance schedule ${\\beta_1, \\beta_2, ..., \\beta_T}$ that controls how much noise is added to the data at each `timestep`. The choice of the variance schedule can have a significant impact on the performance of the model. In this implementation, we use a linear variance schedule, where the variance increases linearly from a small value `start` to a larger value `end` over a fixed number of `timesteps`."
      ],
      "metadata": {
        "id": "yvLjgyArvf8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear variance schedule\n",
        "def linear_beta_schedule(timesteps, start=0.0001, end=0.02):\n",
        "    \"\"\" return timesteps evenly spaced values from start to stop \"\"\"\n",
        "\n",
        "    ## TO DO\n",
        "\n",
        "    return beta_schedule"
      ],
      "metadata": {
        "id": "0pvvFHZZZSbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define $\\alpha_t := 1-\\beta_t$ and $\\overline{\\alpha_t} = \\prod_{i=1}^{t} \\alpha_i$ so that we can sample forward directly at timestep t from the original data $\\mathbf{x}_0$ :\n",
        "$$q(\\mathbf{x}_{1:T}|\\mathbf{x}_0) = \\prod_{t=1}^{T}q(\\mathbf{x}_t|\\mathbf{x}_{t-1})$$\n",
        "$$ q(\\mathbf{x}_t| \\mathbf{x}_0) = \\mathcal{N}(\\mathbf{x}_t;\\sqrt{\\overline{\\alpha_t}} \\mathbf{x}_0, (1-\\overline{\\alpha_t})I)$$"
      ],
      "metadata": {
        "id": "EhGikO_OxFf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_alphas(betas) :\n",
        "  \"\"\" return the alphas as defined above \"\"\"\n",
        "\n",
        "  ## TO DO\n",
        "\n",
        "  return alphas\n",
        "\n",
        "\n",
        "def create_alphas_cumprod(alphas) :\n",
        "  \"\"\" return the cumulative product of the alphas (alpha bar) \"\"\"\n",
        "\n",
        "  ## TO DO\n",
        "\n",
        "  return alphas_cumprod\n",
        "\n",
        "# NB : We break down the construction of the alpha_cumprod in 2 steps as we will\n",
        "# need the intermediate alphas values for the backward process"
      ],
      "metadata": {
        "id": "nahu7oZCqHr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Define linear variance schedule\n",
        "betas = linear_beta_schedule(timesteps=T)\n",
        "\n",
        "## Pre-calculate different terms for closed form\n",
        "\n",
        "alphas = create_alphas(betas)\n",
        "alphas_cumprod = create_alphas_cumprod(alphas)\n",
        "\n",
        "# cumulative product of the alpha values up to the previous timestep\n",
        "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "\n",
        "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
        "\n",
        "# Variance of the noise that was added at the current timestep, given the noisy data at the current and previous timestep\n",
        "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)"
      ],
      "metadata": {
        "id": "zJZPTTrFctdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot the cumulative product of the alphas (alpha bar)\n",
        "## TO DO"
      ],
      "metadata": {
        "id": "ip2VgAc7dBlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1) What do $\\overline{\\alpha_t}$ represent for the forward process at step `t` ?"
      ],
      "metadata": {
        "id": "AdQO6aYH2gwn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A1)"
      ],
      "metadata": {
        "id": "k6ARRQq320h1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Define the forward process function\n",
        "\n",
        "def get_index_from_list(vals, t, x_shape):\n",
        "    \"\"\"\n",
        "    Returns a specific index t of a passed list of values vals\n",
        "    while considering the batch dimension.\n",
        "    \"\"\"\n",
        "    batch_size = t.shape[0]\n",
        "    out = vals.gather(-1, t.cpu()) # alpha bar for values t for instance\n",
        "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(device)\n",
        "\n",
        "def forward_diffusion_sample(x_0, t):\n",
        "    \"\"\"\n",
        "    Takes an image and a timestep as input and\n",
        "    returns the noisy version of it\n",
        "    \"\"\"\n",
        "    noise = torch.randn_like(x_0)\n",
        "    sqrt_alphas_cumprod_t = get_index_from_list(sqrt_alphas_cumprod, t, x_0.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
        "        sqrt_one_minus_alphas_cumprod, t, x_0.shape\n",
        "    )\n",
        "    # mean + variance\n",
        "    return sqrt_alphas_cumprod_t.to(device) * x_0.to(device) \\\n",
        "    + sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device), noise.to(device)"
      ],
      "metadata": {
        "id": "qWw50ui9IZ5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Prepare the MNIST dataset\n",
        "\n",
        "def get_subset(dataset, ratio):\n",
        "    \"\"\"\n",
        "    Returns a subset of the dataset containing a specified ratio of examples.\n",
        "    \"\"\"\n",
        "    indices = list(range(len(dataset)))\n",
        "    subset_size = max(1, int(len(dataset) * ratio))\n",
        "    subset_indices = indices[:subset_size]\n",
        "    return torch.utils.data.Subset(dataset, subset_indices)\n",
        "\n",
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Pad((2, 2, 2, 2), fill=0, padding_mode='constant'),  # Add padding to obtain 32x32 images\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Download and load the MNIST dataset\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainset = get_subset(trainset, ratio=train_ratio)  # Use only train_ratio of the training set for faster training during debugging\n",
        "print(f'Length of training set: {len(trainset)}')\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "print(f'Length of test set: {len(testset)}')\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, drop_last=True)"
      ],
      "metadata": {
        "id": "b6Pz_oPnWxhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Noise an image\n",
        "for batch_nbr, (imgs, labels) in enumerate(trainloader):\n",
        "    t = torch.randint(0, T, (imgs.shape[0],))\n",
        "    print(\"Noising steps list :\\n\", t)\n",
        "    noisy_imgs, noise = forward_diffusion_sample(imgs, t)\n",
        "    plot_grid(noisy_imgs)\n",
        "    break"
      ],
      "metadata": {
        "id": "2fUPyJghdoUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should notice that the bigger the noising timestep is, the noisier the image will be"
      ],
      "metadata": {
        "id": "Jktu4f6C7qV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## U-Net architecture\n"
      ],
      "metadata": {
        "id": "9VZEAJsJ9tCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Let's now define the UNet architecture\n",
        "## U-Net model taken from the very clean pytorch implementation of milesial\n",
        "## https://github.com/milesial/Pytorch-UNet\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, time_emb_dim, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.time_mlp = nn.Sequential(nn.Linear(time_emb_dim, mid_channels), nn.ReLU())\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        x = self.conv1(x)\n",
        "        t = self.time_mlp(t)\n",
        "        t = t[(..., ) + (None, ) * 2]\n",
        "        x = x + t # We add the diffusion step embedding to the image embedding\n",
        "        x = self.conv2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, time_emb_dim):\n",
        "        super().__init__()\n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "        self.conv = DoubleConv(in_channels, out_channels, time_emb_dim)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        return self.maxpool(self.conv(x, t))\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, time_emb_dim, bilinear=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, time_emb_dim, in_channels // 2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, time_emb_dim)\n",
        "\n",
        "    def forward(self, x1, x2, t):\n",
        "        x1 = self.up(x1)\n",
        "\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x, t)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "kYe8Z50vmSG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Full assembly of the parts to form the complete network\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, bilinear=False):\n",
        "        super(UNet, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        # Time embedding\n",
        "        self.time_mlp = nn.Sequential(\n",
        "                SinusoidalPositionEmbeddings(dim=32),\n",
        "                nn.Linear(32, 32),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "\n",
        "        self.inc = (DoubleConv(self.in_channels, 64, time_emb_dim=32))\n",
        "        self.down1 = (Down(64, 128, time_emb_dim=32))\n",
        "        self.down2 = (Down(128, 256, time_emb_dim=32))\n",
        "        self.down3 = (Down(256, 512, time_emb_dim=32))\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = (Down(512, 1024 // factor, time_emb_dim=32))\n",
        "        self.up1 = (Up(1024, 512 // factor, time_emb_dim=32, bilinear=bilinear))\n",
        "        self.up2 = (Up(512, 256 // factor, time_emb_dim=32, bilinear=bilinear))\n",
        "        self.up3 = (Up(256, 128 // factor, time_emb_dim=32, bilinear=bilinear))\n",
        "        self.up4 = (Up(128, 64, time_emb_dim=32, bilinear=bilinear))\n",
        "        self.outc = (OutConv(64, self.out_channels))\n",
        "\n",
        "    def forward(self, x, timestep):\n",
        "        t = self.time_mlp(timestep)\n",
        "        x1 = self.inc(x, t)\n",
        "        x2 = self.down1(x1, t)\n",
        "        x3 = self.down2(x2, t)\n",
        "        x4 = self.down3(x3, t)\n",
        "        x5 = self.down4(x4, t)\n",
        "        x = self.up1(x5, x4, t)\n",
        "        x = self.up2(x, x3, t)\n",
        "        x = self.up3(x, x2, t)\n",
        "        x = self.up4(x, x1, t)\n",
        "        logits = self.outc(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "TjXg1JnOmPCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Sampling function for generation\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_plot_reconstruction(nbr_imgs):\n",
        "    # Sample noise\n",
        "    img = torch.randn((10, 1, img_size, img_size), device=device)\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.axis('off')\n",
        "    stepsize = int(T/nbr_imgs)\n",
        "\n",
        "    for i in range(0,T)[::-1]:\n",
        "        t = torch.full((img.shape[0],), i, device=device, dtype=torch.long)\n",
        "\n",
        "        betas_t = get_index_from_list(betas, t, img.shape)\n",
        "        sqrt_one_minus_alphas_cumprod_t = get_index_from_list(sqrt_one_minus_alphas_cumprod, t, img.shape)\n",
        "        sqrt_recip_alphas_t = get_index_from_list(sqrt_recip_alphas, t, img.shape)\n",
        "\n",
        "        # Call model (current image - noise prediction)\n",
        "        model_mean = sqrt_recip_alphas_t * (img - betas_t * model(img, t) / sqrt_one_minus_alphas_cumprod_t)\n",
        "        posterior_variance_t = get_index_from_list(posterior_variance, t, img.shape)\n",
        "\n",
        "        if t[0] == 0:\n",
        "            img = model_mean\n",
        "        else:\n",
        "            noise = torch.randn_like(img)\n",
        "            img = model_mean + torch.sqrt(posterior_variance_t) * noise\n",
        "\n",
        "        # To maintain the natural range of the distribution\n",
        "        img = torch.clamp(img, -1.0, 1.0)\n",
        "        if i % stepsize == 0: # at diffusion step i\n",
        "          for j in range(img.shape[0]) : # for each img of batch\n",
        "            plt.subplot(10, nbr_imgs, int((T-i)/stepsize) + j*nbr_imgs)\n",
        "            show_images(img[j].detach().cpu())\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "lR-zs_i8uSM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Initialisation and visualisation of the model\n",
        "\n",
        "model = UNet(1, 1) # 1 input channel and 1 output channel for MNIST data (greyscale images)\n",
        "model.to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "dC5DqY61wLfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2) How many parameters does this model have ?"
      ],
      "metadata": {
        "id": "MFH3pMNZFE3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A2)"
      ],
      "metadata": {
        "id": "IKORTuFUEgLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TO DO"
      ],
      "metadata": {
        "id": "hTna_2tjFDdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model and visualise generated samples and training loss"
      ],
      "metadata": {
        "id": "4J6w87MvFXux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Define Adam optimizer and MSE loss\n",
        "\n",
        "## TO DO\n",
        "\n",
        "train_losses = []"
      ],
      "metadata": {
        "id": "1ZHwEVvvsiw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    for step, (imgs_batch, _) in enumerate(trainloader): # imgs_batch is for the\n",
        "    # MNIST images and _ is for the labels which are the corresponding number\n",
        "    # on the images but we don't need them here (it's useful for classification\n",
        "    # tasks for instance)\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Sample a random timestep between 0 and T-1 of shape batch_size\n",
        "      # It has to be of type \"long integer\"\n",
        "      # Don't forget to bring it to GPU (.to(device))\n",
        "      t = ## TO DO\n",
        "\n",
        "      # Compute the noised images and the corresponding added noise\n",
        "      # (deterministic process)\n",
        "      x_noisy, noise = ## TO DO\n",
        "\n",
        "      # Predict the added noise with the model\n",
        "      noise_pred = ## TO DO\n",
        "\n",
        "      # Compute the loss\n",
        "      train_loss = ## TO DO\n",
        "\n",
        "      # Backward pass and update weights\n",
        "      train_loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    train_losses.append(train_loss.item())\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "      print(\"Epoch\", epoch, \"Loss:\", train_loss.item(), \"Time elapsed :\", np.round(time.time() - start, 2), \"secs\")\n",
        "      sample_plot_reconstruction(10)"
      ],
      "metadata": {
        "id": "bpN_LKYwuLx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses)"
      ],
      "metadata": {
        "id": "e_9Qu-PSm3Zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once your training loop properly works you can try to train the model with more training data to have better quality generation (change `train_ratio` in the hyperparameters)"
      ],
      "metadata": {
        "id": "i19cuuYHPq0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conditional Generation\n",
        "We will now add the embeddings of MNIST labels (just like we already do with time embeddings for diffusion) to transform our model into a conditional model. These embeddings are learned throughout training. Another approach would be to add fixed embeddings (one-hot encoding for instance). <br><br>\n",
        "In the following, your task is to edit the architecture of the model to take into account these MNIST label embeddings."
      ],
      "metadata": {
        "id": "mzdz205b0OZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, time_emb_dim, label_emb_dim, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.time_mlp = nn.Sequential(nn.Linear(time_emb_dim, mid_channels), nn.ReLU())\n",
        "        self.label_mlp = nn.Sequential(nn.Linear(label_emb_dim, mid_channels), nn.ReLU())\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t, l):\n",
        "\n",
        "      ## TO DO\n",
        "      # Add the diffusion step embedding and the label embedding to the image embedding\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, time_emb_dim, label_emb_dim):\n",
        "        super().__init__()\n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "        self.conv = DoubleConv(in_channels, out_channels, time_emb_dim, label_emb_dim)\n",
        "\n",
        "    def forward(self, x, t, l):\n",
        "        return       ## TO DO (take into account label embedding)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, time_emb_dim, label_emb_dim, bilinear=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, time_emb_dim, label_emb_dim, in_channels // 2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, time_emb_dim, label_emb_dim)\n",
        "\n",
        "    def forward(self, x1, x2, t, l):\n",
        "        x1 = self.up(x1)\n",
        "\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return ## TO DO (take into account label embedding)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "7Bs1H2EqQAxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## We specify the label embedding dimension and add the label embedding\n",
        "## For the DoubleConv layers\n",
        "\n",
        "class Conditional_UNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, bilinear=False):\n",
        "        super(Conditional_UNet, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        # Time embedding\n",
        "        self.time_mlp = nn.Sequential(\n",
        "                SinusoidalPositionEmbeddings(dim=32),\n",
        "                nn.Linear(32, 32),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "\n",
        "        # Label embedding for conditional generation\n",
        "        self.label_embeddings = nn.Embedding(num_embeddings=10, embedding_dim=32)\n",
        "\n",
        "        self.inc = (DoubleConv(self.in_channels, 64, time_emb_dim=32, label_emb_dim=32))\n",
        "        self.down1 = (Down(64, 128, time_emb_dim=32, label_emb_dim=32))\n",
        "        self.down2 = (Down(128, 256, time_emb_dim=32, label_emb_dim=32))\n",
        "        self.down3 = (Down(256, 512, time_emb_dim=32, label_emb_dim=32))\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = (Down(512, 1024 // factor, time_emb_dim=32, label_emb_dim=32))\n",
        "        self.up1 = (Up(1024, 512 // factor, time_emb_dim=32, label_emb_dim=32, bilinear=bilinear))\n",
        "        self.up2 = (Up(512, 256 // factor, time_emb_dim=32, label_emb_dim=32, bilinear=bilinear))\n",
        "        self.up3 = (Up(256, 128 // factor, time_emb_dim=32, label_emb_dim=32, bilinear=bilinear))\n",
        "        self.up4 = (Up(128, 64, time_emb_dim=32, label_emb_dim=32, bilinear=bilinear))\n",
        "        self.outc = (OutConv(64, self.out_channels))\n",
        "\n",
        "    def forward(self, x, timestep, labels):\n",
        "        ## TO DO (take into account label embedding)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "fIDiPnkE0gbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## New sampling function for generation\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_plot_reconstruction_conditional(labels_list):\n",
        "    # Sample noise\n",
        "    img = torch.randn((len(labels_list), 1, img_size, img_size), device=device)\n",
        "    plt.figure(figsize=(8,len(labels_list)))\n",
        "    plt.axis('off')\n",
        "    stepsize = int(T/10)\n",
        "    #labels = torch.tensor([i for i in range(img.shape[0])]).to(device)\n",
        "    labels = torch.tensor(labels_list).to(device)\n",
        "\n",
        "    for i in range(0,T)[::-1]:\n",
        "        t = torch.full((img.shape[0],), i, device=device, dtype=torch.long)\n",
        "\n",
        "        betas_t = get_index_from_list(betas, t, img.shape)\n",
        "        sqrt_one_minus_alphas_cumprod_t = get_index_from_list(sqrt_one_minus_alphas_cumprod, t, img.shape)\n",
        "        sqrt_recip_alphas_t = get_index_from_list(sqrt_recip_alphas, t, img.shape)\n",
        "\n",
        "        # Call model (current image - noise prediction)\n",
        "        model_mean = sqrt_recip_alphas_t * (img - betas_t * model(img, t, labels) / sqrt_one_minus_alphas_cumprod_t)\n",
        "        posterior_variance_t = get_index_from_list(posterior_variance, t, img.shape)\n",
        "\n",
        "        if t[0] == 0:\n",
        "            img = model_mean\n",
        "        else:\n",
        "            noise = torch.randn_like(img)\n",
        "            img = model_mean + torch.sqrt(posterior_variance_t) * noise\n",
        "\n",
        "        # To maintain the natural range of the distribution\n",
        "        img = torch.clamp(img, -1.0, 1.0)\n",
        "        if i % stepsize == 0: # at diffusion step i\n",
        "          for j in range(img.shape[0]) : # for each img of batch\n",
        "              plt.subplot(len(labels_list), 10, int((T-i)/stepsize) + j*10)\n",
        "              show_images(img[j].detach().cpu())\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "d8tiFmYH0gZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Initialisation and visualisation of the model\n",
        "\n",
        "model = Conditional_UNet(1, 1) # 1 input channel and 1 output channel for MNIST data (greyscale images)\n",
        "model.to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "mW3V_6b-0gTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Define Adam optimizer and MSE loss\n",
        "\n",
        "## TO DO\n",
        "\n",
        "train_losses = []"
      ],
      "metadata": {
        "id": "y0RCGTUn0pUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    for step, (imgs_batch, labels_batch) in enumerate(trainloader): # imgs_batch is for the\n",
        "      # MNIST images and labels_batch is for the labels which are the corresponding number\n",
        "      # on the images\n",
        "      labels_batch = labels_batch.to(device)\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Sample a random timestep between 0 and T-1 of shape batch_size\n",
        "      # It has to be of type \"long integer\"\n",
        "      # Don't forget to bring it to GPU (.to(device))\n",
        "      t = ## TO DO\n",
        "\n",
        "      # Compute the noised images and the corresponding added noise\n",
        "      # (deterministic process)\n",
        "      x_noisy, noise = ## TO DO\n",
        "\n",
        "      # Predict the added noise with the model\n",
        "      # --> Don't forget to add the new label argument for conditional generation\n",
        "      noise_pred = ## TO DO\n",
        "\n",
        "      # Compute the loss\n",
        "      train_loss = ## TO DO\n",
        "\n",
        "      # Backward pass and update weights\n",
        "      train_loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    train_losses.append(train_loss.item())\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "      print(\"Epoch\", epoch, \"Loss:\", train_loss.item(), \"Time elapsed :\", np.round(time.time() - start, 2), \"secs\")\n",
        "      labels_list = [1,2,3] # Choose what numbers you would like to generate (you can increase the length of the list as you wish)\n",
        "      sample_plot_reconstruction_conditional(labels_list)"
      ],
      "metadata": {
        "id": "4qxPcXmZ0rSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "i4qPT3nO0t-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note : We  trained this model from scratch again to make the diffusion process conditional using a classifier-free guidance approach. Another approach would be to perform classifier guidance which boils down to train a separate classifier and edit the sampling step during generation by adding the gradient of the log probability with respect to the label ($x_{t−1}​=f(x_{t​},t)+α∇x_{t​}​logP(y∣x_{t}​)$). <br> <br> Pros of this methods are :\n",
        "\n",
        "1.   no need to retrain the diffusion model from scratch\n",
        "2.   you can leverage an existing pretrained classifier for conditionning\n",
        "3.   you usually get better quality results with this approach\n",
        "\n",
        "You can try to implement that yourself if you wish :) (Bonus)"
      ],
      "metadata": {
        "id": "RSzcd5MULjaS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yqG0pn9WMUmG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}